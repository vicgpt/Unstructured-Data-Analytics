{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "96bf1ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\gupta\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\gupta\\anaconda3\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\gupta\\anaconda3\\lib\\site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: click in c:\\users\\gupta\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\gupta\\anaconda3\\lib\\site-packages (from nltk) (2021.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize, Counter\n",
    "from nltk.corpus import stopwords\n",
    "import itertools\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe75d344",
   "metadata": {},
   "source": [
    "### Objective - Use the raw captions from scrapped data and convert it into a list of words and tf/idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d1b909c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>img_url</th>\n",
       "      <th>caption</th>\n",
       "      <th>n_likes_1000</th>\n",
       "      <th>n_comments</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://scontent-waw1-1.cdninstagram.com/v/t51...</td>\n",
       "      <td>‚Äú100% of myself is nothing compared to 1% of t...</td>\n",
       "      <td>218k</td>\n",
       "      <td>9590</td>\n",
       "      <td>1 day ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://scontent-waw1-1.cdninstagram.com/v/t51...</td>\n",
       "      <td>Meet @azusa25nigo, the founder of @skate_girls...</td>\n",
       "      <td>77k</td>\n",
       "      <td>62680</td>\n",
       "      <td>2 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://scontent-waw1-1.cdninstagram.com/v/t51...</td>\n",
       "      <td>It takes courage to take the first step üèÉ. Jus...</td>\n",
       "      <td>232k</td>\n",
       "      <td>46169</td>\n",
       "      <td>2 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>https://scontent-waw1-1.cdninstagram.com/v/t51...</td>\n",
       "      <td>‚ÄúThe climate crisis is affecting my sport and ...</td>\n",
       "      <td>158k</td>\n",
       "      <td>86967</td>\n",
       "      <td>6 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://scontent-waw1-1.cdninstagram.com/v/t51...</td>\n",
       "      <td>‚ÄúPeople like to tell us what we can and can‚Äôt ...</td>\n",
       "      <td>251k</td>\n",
       "      <td>67604</td>\n",
       "      <td>1 week ago</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            img_url  \\\n",
       "0           0  https://scontent-waw1-1.cdninstagram.com/v/t51...   \n",
       "1           1  https://scontent-waw1-1.cdninstagram.com/v/t51...   \n",
       "2           2  https://scontent-waw1-1.cdninstagram.com/v/t51...   \n",
       "3           3  https://scontent-waw1-1.cdninstagram.com/v/t51...   \n",
       "4           4  https://scontent-waw1-1.cdninstagram.com/v/t51...   \n",
       "\n",
       "                                             caption n_likes_1000  n_comments  \\\n",
       "0  ‚Äú100% of myself is nothing compared to 1% of t...         218k        9590   \n",
       "1  Meet @azusa25nigo, the founder of @skate_girls...          77k       62680   \n",
       "2  It takes courage to take the first step üèÉ. Jus...         232k       46169   \n",
       "3  ‚ÄúThe climate crisis is affecting my sport and ...         158k       86967   \n",
       "4  ‚ÄúPeople like to tell us what we can and can‚Äôt ...         251k       67604   \n",
       "\n",
       "          age  \n",
       "0   1 day ago  \n",
       "1  2 days ago  \n",
       "2  2 days ago  \n",
       "3  6 days ago  \n",
       "4  1 week ago  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the data\n",
    "nike_data = pd.read_csv('nike_data.csv')\n",
    "nike_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8847269d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(624, 6)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nike_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fd73c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to lemmatize all words in captions\n",
    "def lemmatization(text):\n",
    "    text = nlp(text)\n",
    "    text_lemma = [word.lemma_ for word in text]\n",
    "    return \" \".join(text_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7a0f25b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean caption obtained from scrapper\n",
    "def wrangle(text):\n",
    "    text = text.replace('‚Äú', '\"').replace('‚Äù','\"').replace('‚Äô', \"'\")\n",
    "    printable = set(string.printable)\n",
    "    text = ''.join(filter(lambda x: x in printable, text))\n",
    "   \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f885cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove stop words and punctuations from the list of caption words\n",
    "def remove_stopwords(lst):\n",
    "    return [word for word in lst if \n",
    "            ( (word not in stopwords.words()) &\n",
    "            (word not in list(string.punctuation)) &\n",
    "            (word not in list(string.digits)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "25a538dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the cleaning functions\n",
    "def caption_cleaning(data):\n",
    "    # subsetting captions only\n",
    "    captions = data[['caption']]\n",
    "    captions['caption'] = captions['caption'].astype(str).str.strip()\n",
    "    captions['caption'] = captions['caption'].map(lambda s: wrangle(s))\n",
    "    captions['caption_lemma'] = captions['caption'].map(lemmatization)\n",
    "\n",
    "    # creating caption list\n",
    "    captions['caption_list'] = captions['caption_lemma'].map(\n",
    "        lambda row: word_tokenize(row.lower()))\n",
    "    \n",
    "    # removing stop words and punctuation\n",
    "    captions['caption_list'] = captions['caption_list'].map(lambda row: remove_stopwords(row))\n",
    "    \n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "47537216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tf idf dataframe\n",
    "def tf_idf(col):\n",
    "    # creating tf-idf vector\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(col.values)\n",
    "    columns = vectorizer.get_feature_names()\n",
    "\n",
    "    # creating tf idf df\n",
    "    tf_idf_df = pd.DataFrame(X.toarray(), columns=columns)\n",
    "    \n",
    "    return tf_idf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8d0c2320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating tf-idf vectors\n",
    "def caption_tf_idf(captions):\n",
    "    # creating a list of caption words\n",
    "    caption_words_list = []\n",
    "    for l in list(captions['caption_list'].values):\n",
    "        caption_words_list = caption_words_list + l\n",
    "\n",
    "    # removing duplicates \n",
    "    caption_words_list = list(set(caption_words_list))\n",
    "\n",
    "    # removing words with length <= 2\n",
    "    caption_words_list = [w for w in caption_words_list if len(w) > 2]\n",
    "\n",
    "    # converting the text to list\n",
    "    captions['caption_cleaned'] = captions['caption_list'].map(lambda lst: ' '.join(lst))\n",
    "    \n",
    "    # get tf idf vec\n",
    "    caption_tf_idf = tf_idf(captions['caption_cleaned'])\n",
    "    caption_words_list = list(set(caption_words_list).intersection(set(caption_tf_idf.columns)))\n",
    "    caption_tf_idf = caption_tf_idf[caption_words_list]\n",
    "    \n",
    "    return caption_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b3660656",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-95-2a8e923f7601>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  captions['caption'] = captions['caption'].astype(str).str.strip()\n",
      "<ipython-input-95-2a8e923f7601>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  captions['caption'] = captions['caption'].map(lambda s: wrangle(s))\n",
      "<ipython-input-95-2a8e923f7601>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  captions['caption_lemma'] = captions['caption'].map(lemmatization)\n",
      "<ipython-input-95-2a8e923f7601>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  captions['caption_list'] = captions['caption_lemma'].map(\n",
      "<ipython-input-95-2a8e923f7601>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  captions['caption_list'] = captions['caption_list'].map(lambda row: remove_stopwords(row))\n"
     ]
    }
   ],
   "source": [
    "nike_captions = caption_cleaning(nike_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "73632cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caption</th>\n",
       "      <th>caption_lemma</th>\n",
       "      <th>caption_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"100% of myself is nothing compared to 1% of t...</td>\n",
       "      <td>\" 100 % of myself be nothing compare to 1 % of...</td>\n",
       "      <td>[``, 100, nothing, compare, team, ``, kipchoge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Meet @azusa25nigo, the founder of @skate_girls...</td>\n",
       "      <td>Meet @azusa25nigo , the founder of @skate_girl...</td>\n",
       "      <td>[meet, azusa25nigo, founder, skate_girls_snap,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It takes courage to take the first step . Just...</td>\n",
       "      <td>it take courage to take the first step . just ...</td>\n",
       "      <td>[courage, first, step, ask, azusa25nigo, azusa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"The climate crisis is affecting my sport and ...</td>\n",
       "      <td>\" the climate crisis be affect my sport and at...</td>\n",
       "      <td>[``, climate, crisis, affect, sport, athlete, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"People like to tell us what we can and can't ...</td>\n",
       "      <td>\" People like to tell we what we can and ca n'...</td>\n",
       "      <td>[``, people, like, tell, n't, n't, hear, real,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>Quick stop at Niketown London. Had to design a...</td>\n",
       "      <td>quick stop at Niketown London . have to design...</td>\n",
       "      <td>[quick, stop, niketown, london, design, nikeid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>Flyknit research at @1948London.</td>\n",
       "      <td>Flyknit research at @1948London .</td>\n",
       "      <td>[flyknit, research, 1948london]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>A little testing turned into a full workout fo...</td>\n",
       "      <td>a little testing turn into a full workout for ...</td>\n",
       "      <td>[little, testing, turn, full, workout, michell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>A sport center in the center of sport. The Nik...</td>\n",
       "      <td>a sport center in the center of sport . the Ni...</td>\n",
       "      <td>[sport, center, center, sport, nike+, fuelstat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>Inspired by everyones NikeFuel, we had to get ...</td>\n",
       "      <td>inspire by everyone NikeFuel , we have to get ...</td>\n",
       "      <td>[inspire, everyone, nikefuel, get, moonlight, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>624 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               caption  \\\n",
       "0    \"100% of myself is nothing compared to 1% of t...   \n",
       "1    Meet @azusa25nigo, the founder of @skate_girls...   \n",
       "2    It takes courage to take the first step . Just...   \n",
       "3    \"The climate crisis is affecting my sport and ...   \n",
       "4    \"People like to tell us what we can and can't ...   \n",
       "..                                                 ...   \n",
       "619  Quick stop at Niketown London. Had to design a...   \n",
       "620                   Flyknit research at @1948London.   \n",
       "621  A little testing turned into a full workout fo...   \n",
       "622  A sport center in the center of sport. The Nik...   \n",
       "623  Inspired by everyones NikeFuel, we had to get ...   \n",
       "\n",
       "                                         caption_lemma  \\\n",
       "0    \" 100 % of myself be nothing compare to 1 % of...   \n",
       "1    Meet @azusa25nigo , the founder of @skate_girl...   \n",
       "2    it take courage to take the first step . just ...   \n",
       "3    \" the climate crisis be affect my sport and at...   \n",
       "4    \" People like to tell we what we can and ca n'...   \n",
       "..                                                 ...   \n",
       "619  quick stop at Niketown London . have to design...   \n",
       "620                  Flyknit research at @1948London .   \n",
       "621  a little testing turn into a full workout for ...   \n",
       "622  a sport center in the center of sport . the Ni...   \n",
       "623  inspire by everyone NikeFuel , we have to get ...   \n",
       "\n",
       "                                          caption_list  \n",
       "0    [``, 100, nothing, compare, team, ``, kipchoge...  \n",
       "1    [meet, azusa25nigo, founder, skate_girls_snap,...  \n",
       "2    [courage, first, step, ask, azusa25nigo, azusa...  \n",
       "3    [``, climate, crisis, affect, sport, athlete, ...  \n",
       "4    [``, people, like, tell, n't, n't, hear, real,...  \n",
       "..                                                 ...  \n",
       "619  [quick, stop, niketown, london, design, nikeid...  \n",
       "620                    [flyknit, research, 1948london]  \n",
       "621  [little, testing, turn, full, workout, michell...  \n",
       "622  [sport, center, center, sport, nike+, fuelstat...  \n",
       "623  [inspire, everyone, nikefuel, get, moonlight, ...  \n",
       "\n",
       "[624 rows x 3 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nike_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "27ad59bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-107-aca970e00572>:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  captions['caption_cleaned'] = captions['caption_list'].map(lambda lst: ' '.join(lst))\n"
     ]
    }
   ],
   "source": [
    "nike_tf_idf = caption_tf_idf(nike_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f3d6a737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>back</th>\n",
       "      <th>fast</th>\n",
       "      <th>passionate</th>\n",
       "      <th>train</th>\n",
       "      <th>asphalt</th>\n",
       "      <th>girls</th>\n",
       "      <th>starting</th>\n",
       "      <th>power</th>\n",
       "      <th>paulaaloe</th>\n",
       "      <th>marlen_esparza</th>\n",
       "      <th>...</th>\n",
       "      <th>breathe</th>\n",
       "      <th>statue</th>\n",
       "      <th>paralympic</th>\n",
       "      <th>thibautgrevet</th>\n",
       "      <th>work</th>\n",
       "      <th>sand</th>\n",
       "      <th>prelive</th>\n",
       "      <th>goals</th>\n",
       "      <th>rug</th>\n",
       "      <th>mat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.114277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.274966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133194</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>624 rows √ó 2853 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     back  fast  passionate  train  asphalt     girls  starting  power  \\\n",
       "0     0.0   0.0         0.0    0.0      0.0  0.000000       0.0    0.0   \n",
       "1     0.0   0.0         0.0    0.0      0.0  0.274966       0.0    0.0   \n",
       "2     0.0   0.0         0.0    0.0      0.0  0.133194       0.0    0.0   \n",
       "3     0.0   0.0         0.0    0.0      0.0  0.000000       0.0    0.0   \n",
       "4     0.0   0.0         0.0    0.0      0.0  0.000000       0.0    0.0   \n",
       "..    ...   ...         ...    ...      ...       ...       ...    ...   \n",
       "619   0.0   0.0         0.0    0.0      0.0  0.000000       0.0    0.0   \n",
       "620   0.0   0.0         0.0    0.0      0.0  0.000000       0.0    0.0   \n",
       "621   0.0   0.0         0.0    0.0      0.0  0.000000       0.0    0.0   \n",
       "622   0.0   0.0         0.0    0.0      0.0  0.000000       0.0    0.0   \n",
       "623   0.0   0.0         0.0    0.0      0.0  0.000000       0.0    0.0   \n",
       "\n",
       "     paulaaloe  marlen_esparza  ...  breathe  statue  paralympic  \\\n",
       "0          0.0             0.0  ...      0.0     0.0         0.0   \n",
       "1          0.0             0.0  ...      0.0     0.0         0.0   \n",
       "2          0.0             0.0  ...      0.0     0.0         0.0   \n",
       "3          0.0             0.0  ...      0.0     0.0         0.0   \n",
       "4          0.0             0.0  ...      0.0     0.0         0.0   \n",
       "..         ...             ...  ...      ...     ...         ...   \n",
       "619        0.0             0.0  ...      0.0     0.0         0.0   \n",
       "620        0.0             0.0  ...      0.0     0.0         0.0   \n",
       "621        0.0             0.0  ...      0.0     0.0         0.0   \n",
       "622        0.0             0.0  ...      0.0     0.0         0.0   \n",
       "623        0.0             0.0  ...      0.0     0.0         0.0   \n",
       "\n",
       "     thibautgrevet      work  sand  prelive  goals  rug  mat  \n",
       "0              0.0  0.114277   0.0      0.0    0.0  0.0  0.0  \n",
       "1              0.0  0.000000   0.0      0.0    0.0  0.0  0.0  \n",
       "2              0.0  0.000000   0.0      0.0    0.0  0.0  0.0  \n",
       "3              0.0  0.083238   0.0      0.0    0.0  0.0  0.0  \n",
       "4              0.0  0.000000   0.0      0.0    0.0  0.0  0.0  \n",
       "..             ...       ...   ...      ...    ...  ...  ...  \n",
       "619            0.0  0.000000   0.0      0.0    0.0  0.0  0.0  \n",
       "620            0.0  0.000000   0.0      0.0    0.0  0.0  0.0  \n",
       "621            0.0  0.000000   0.0      0.0    0.0  0.0  0.0  \n",
       "622            0.0  0.000000   0.0      0.0    0.0  0.0  0.0  \n",
       "623            0.0  0.000000   0.0      0.0    0.0  0.0  0.0  \n",
       "\n",
       "[624 rows x 2853 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nike_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ba76b8cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['back', 'fast', 'passionate', 'train', 'asphalt', 'girls', 'starting',\n",
       "       'power', 'paulaaloe', 'marlen_esparza', 'purpose', 'improbable',\n",
       "       'weekday', 'crowd', 'technically', 'hike', 'nothing', 'win', 'elina',\n",
       "       'shine', 'werundc', 'dedication', 'igtv', 'makeitcount',\n",
       "       'kevinhart4real', 'baltimore', 'paula', 'nadal', 'accordance', 'betrue',\n",
       "       'easymoneysniper', 'reclaim', 'hour', 'lunar2', 'defeat', 'aim', 'keep',\n",
       "       'niketennis', 'playinside', 'skater', 'tree', '100', 'overlook',\n",
       "       'conclusion', 'ramla', 'anyway', 'nikesnowboardings', 'courtballistic',\n",
       "       'java', 'lunarglide', 'believer', 'paulo', 'baylor', 'turf', 'ussoccer',\n",
       "       'playoff', 'brigidkosgei', 'gurlstalk', 'family', 'forge', 'lunarlon',\n",
       "       'stopasianhate', 'niketown', 'strategically', 'adar', 'serena', '1982',\n",
       "       'meet', 'autumn', 'gearup', 'saturdays', 'indian', 'nouf', 'slogan',\n",
       "       'shoe', 'ribbon', 'opponent', 'collide', 'dry', 'soul', 'throw',\n",
       "       'applause', 'organization', 'mardi', 'many', 'laser', 'jmedvedevaj',\n",
       "       'curl', 'glaciere', 'tde', 'fighter', 'historic', 'liquid', 'happen',\n",
       "       'lift', 'hip', 'jacket', 'splash', 'try', 'hit'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nike_tf_idf.columns[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a3a94a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119e38e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e47df94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
