{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bf1ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize, Counter\n",
    "from nltk.corpus import stopwords\n",
    "import itertools\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe75d344",
   "metadata": {},
   "source": [
    "### Objective - Use the raw captions from scrapped data and convert it into a list of words and tf/idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd73c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to lemmatize all words in captions\n",
    "def lemmatization(text):\n",
    "    text = nlp(text)\n",
    "    text_lemma = [word.lemma_ for word in text]\n",
    "    return \" \".join(text_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0f25b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean caption obtained from scrapper\n",
    "def wrangle(text):\n",
    "    text = text.replace('“', '\"').replace('”','\"').replace('’', \"'\")\n",
    "    printable = set(string.printable)\n",
    "    text = ''.join(filter(lambda x: x in printable, text))\n",
    "   \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f885cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove stop words and punctuations from the list of caption words\n",
    "def remove_stopwords(lst):\n",
    "    return [word for word in lst if \n",
    "            ( (word not in stopwords.words()) &\n",
    "            (word not in list(string.punctuation)) &\n",
    "            (word not in list(string.digits)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a538dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the cleaning functions\n",
    "def caption_cleaning(data):\n",
    "    # subsetting captions only\n",
    "    captions = data[['caption']]\n",
    "    captions['caption'] = captions['caption'].astype(str).str.strip()\n",
    "    captions['caption'] = captions['caption'].map(lambda s: wrangle(s))\n",
    "    captions['caption_lemma'] = captions['caption'].map(lemmatization)\n",
    "\n",
    "    # creating caption list\n",
    "    captions['caption_list'] = captions['caption_lemma'].map(\n",
    "        lambda row: word_tokenize(row.lower()))\n",
    "    \n",
    "    # removing stop words and punctuation\n",
    "    captions['caption_list'] = captions['caption_list'].map(lambda row: remove_stopwords(row))\n",
    "    \n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e83e84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tf dataframe\n",
    "def tf(col):\n",
    "    # creating tf-idf vector\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(col.values)\n",
    "    columns = vectorizer.get_feature_names()\n",
    "\n",
    "    # creating tf idf df\n",
    "    tf_idf_df = pd.DataFrame(X.toarray(), columns=columns)\n",
    "    \n",
    "    return tf_idf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47537216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tf idf dataframe\n",
    "def tf_idf(col):\n",
    "    # creating tf-idf vector\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(col.values)\n",
    "    columns = vectorizer.get_feature_names()\n",
    "\n",
    "    # creating tf idf df\n",
    "    tf_idf_df = pd.DataFrame(X.toarray(), columns=columns)\n",
    "    \n",
    "    return tf_idf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4580642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating tf-idf vectors\n",
    "def caption_tf_idf(captions, idf=True):\n",
    "    # creating a list of caption words\n",
    "    caption_words_list = []\n",
    "    for l in list(captions['caption_list'].values):\n",
    "        caption_words_list = caption_words_list + l\n",
    "\n",
    "    # removing duplicates \n",
    "    caption_words_list = list(set(caption_words_list))\n",
    "\n",
    "    # removing words with length <= 2\n",
    "    caption_words_list = [w for w in caption_words_list if len(w) > 2]\n",
    "\n",
    "    # converting the text to list\n",
    "    captions['caption_cleaned'] = captions['caption_list'].map(lambda lst: ' '.join(lst))\n",
    "    \n",
    "    # get tf idf vec\n",
    "    if idf:\n",
    "        caption_tf_idf = tf_idf(captions['caption_cleaned'])\n",
    "    else:\n",
    "        caption_tf_idf = tf(captions['caption_cleaned'])\n",
    "    caption_words_list = list(set(caption_words_list).intersection(set(caption_tf_idf.columns)))\n",
    "    caption_tf_idf = caption_tf_idf[caption_words_list]\n",
    "    \n",
    "    return caption_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b636cddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the cleaning functions\n",
    "def label_cleaning(data):\n",
    "    # subsetting labels only\n",
    "    labels = data[['labels']]\n",
    "    labels['labels'] = labels['labels'].astype(str).str.strip()\n",
    "    labels['labels'] = labels['labels'].map(lambda s: wrangle(s))\n",
    "    labels['labels_lemma'] = labels['labels'].map(lemmatization)\n",
    "\n",
    "    # creating labels list\n",
    "    labels['labels_list'] = labels['labels_lemma'].map(\n",
    "        lambda row: word_tokenize(row.lower()))\n",
    "    \n",
    "    # removing stop words and punctuation: NOT NEEDED FOR LABELS\n",
    "    # labels['labels_list'] = labels['labels_list'].map(lambda row: remove_stopwords(row))\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995a70e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating tf-idf vectors\n",
    "def labels_tf_idf(labels, idf=True):\n",
    "    # creating a list of labels words\n",
    "    labels_words_list = []\n",
    "    for l in list(labels['labels_list'].values):\n",
    "        labels_words_list = labels_words_list + l\n",
    "\n",
    "    # removing duplicates \n",
    "    labels_words_list = list(set(labels_words_list))\n",
    "\n",
    "    # removing words with length <= 2\n",
    "    labels_words_list = [w for w in labels_words_list if len(w) > 2]\n",
    "\n",
    "    # converting the text to list\n",
    "    labels['labels_cleaned'] = labels['labels_list'].map(lambda lst: ' '.join(lst))\n",
    "    \n",
    "    # get tf idf vec\n",
    "    if idf:\n",
    "        labels_tf_idf = tf_idf(labels['labels_cleaned'])\n",
    "    else:\n",
    "        labels_tf_idf = tf(labels['labels_cleaned'])\n",
    "    labels_words_list = list(set(labels_words_list).intersection(set(labels_tf_idf.columns)))\n",
    "    labels_tf_idf = labels_tf_idf[labels_words_list]\n",
    "    \n",
    "    return labels_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a1643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating tf-idf vectors\n",
    "def img_cap_tf_idf(img_cap, idf=True):\n",
    "    # creating a list of img_cap words\n",
    "    words_list = []\n",
    "    for l in list(img_cap['img_cap_list'].values):\n",
    "        words_list = words_list + l\n",
    "\n",
    "    # removing duplicates \n",
    "    words_list = list(set(words_list))\n",
    "\n",
    "    # removing words with length <= 2\n",
    "    words_list = [w for w in words_list if len(w) > 2]\n",
    "\n",
    "    # converting the text to list\n",
    "    img_cap['cleaned'] = img_cap['img_cap_list'].map(lambda lst: ' '.join(lst))\n",
    "    \n",
    "    # get tf idf vec\n",
    "    if idf:\n",
    "        img_cap_tf_idf = tf_idf(img_cap['cleaned'])\n",
    "    else:\n",
    "        img_cap_tf_idf = tf(img_cap['cleaned'])\n",
    "    words_list = list(set(words_list).intersection(set(img_cap_tf_idf.columns)))\n",
    "    img_cap_tf_idf = img_cap_tf_idf[words_list]\n",
    "    \n",
    "    return img_cap_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716cc719",
   "metadata": {},
   "source": [
    "## Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806c99ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data\n",
    "data = pd.read_csv('Nike/nike_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6e0c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246e8ea8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "captions = caption_cleaning(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f62bb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe4d32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_tf_idf_df = caption_tf_idf(captions)\n",
    "captions_tf_idf_df['caption'] = captions['caption']\n",
    "captions_tf_idf_df['caption_list'] = captions['caption_list']\n",
    "captions_tf_idf_df.to_csv('nike_caption_tf_idf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def920ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_tf_df = caption_tf_idf(captions, idf=False)\n",
    "captions_tf_df['caption'] = captions['caption']\n",
    "captions_tf_df['caption_list'] = captions['caption_list']\n",
    "captions_tf_df.to_csv('nike_caption_tf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239353de",
   "metadata": {},
   "source": [
    "## Image labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f927340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_excel('Nike/NikeLabels.xlsx')\n",
    "labels.drop(0, inplace=True, axis=0)\n",
    "labels.reset_index(drop=True, inplace=True)\n",
    "labels.columns = [s.lower() for s in labels.columns]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de26fb04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_labels = label_cleaning(labels)\n",
    "image_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541b6c22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_tf_idf_df = labels_tf_idf(image_labels)\n",
    "image_tf_idf_df['labels'] = image_labels['labels']\n",
    "image_tf_idf_df['labels_list'] = image_labels['labels_list']\n",
    "print(image_tf_idf_df.head())\n",
    "image_tf_idf_df.to_csv('nike_label_tf_idf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349af6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tf_df = labels_tf_idf(image_labels, idf=False)\n",
    "image_tf_df['labels'] = image_labels['labels']\n",
    "image_tf_df['labels_list'] = image_labels['labels_list']\n",
    "print(image_tf_df.head())\n",
    "image_tf_df.to_csv('nike_label_tf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11e8873",
   "metadata": {},
   "source": [
    "## Image + Caption "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584baf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_caption = pd.concat([image_labels[['labels', 'labels_list']], captions[['caption', 'caption_list']]], axis=1)\n",
    "image_caption['img_cap_list'] = image_caption.apply(lambda row: row['labels_list'] + row['caption_list'], axis=1)\n",
    "image_caption.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a70da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check first entry\n",
    "assert len(image_caption.iloc[0, 1]) + len(image_caption.iloc[0, 3]) == len(image_caption.iloc[0, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd3306a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cap_tf_idf_df = img_cap_tf_idf(image_caption)\n",
    "img_cap_tf_idf_df['img_cap_list'] = image_caption['img_cap_list']\n",
    "print(img_cap_tf_idf_df.head())\n",
    "img_cap_tf_idf_df.to_csv('nike_img_cap_tf_idf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dae406",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cap_tf_df = img_cap_tf_idf(image_caption, idf=False)\n",
    "img_cap_tf_df['img_cap_list'] = image_caption['img_cap_list']\n",
    "print(img_cap_tf_df.head())\n",
    "img_cap_tf_df.to_csv('nike_img_cap_tf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5afc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert img_cap_tf_df.sum(axis=1).sum() - captions_tf_df.sum(axis=1).sum() - image_tf_df.sum(axis=1).sum() <= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29d1cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f19fb54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaa9be7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
