{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96bf1ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\gupta\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\gupta\\anaconda3\\lib\\site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: click in c:\\users\\gupta\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\gupta\\anaconda3\\lib\\site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\gupta\\anaconda3\\lib\\site-packages (from nltk) (1.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize, Counter\n",
    "from nltk.corpus import stopwords\n",
    "import itertools\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe75d344",
   "metadata": {},
   "source": [
    "### Objective - Use the raw captions from scrapped data and convert it into a list of words and tf/idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd73c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to lemmatize all words in captions\n",
    "def lemmatization(text):\n",
    "    text = nlp(text)\n",
    "    text_lemma = [word.lemma_ for word in text]\n",
    "    return \" \".join(text_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a0f25b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean caption obtained from scrapper\n",
    "def wrangle(text):\n",
    "    text = text.replace('‚Äú', '\"').replace('‚Äù','\"').replace('‚Äô', \"'\")\n",
    "    printable = set(string.printable)\n",
    "    text = ''.join(filter(lambda x: x in printable, text))\n",
    "   \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f885cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove stop words and punctuations from the list of caption words\n",
    "def remove_stopwords(lst):\n",
    "    return [word for word in lst if \n",
    "            ( (word not in stopwords.words()) &\n",
    "            (word not in list(string.punctuation)) &\n",
    "            (word not in list(string.digits)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25a538dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the cleaning functions\n",
    "def caption_cleaning(data):\n",
    "    # subsetting captions only\n",
    "    captions = data[['caption']]\n",
    "    captions['caption'] = captions['caption'].astype(str).str.strip()\n",
    "    captions['caption'] = captions['caption'].map(lambda s: wrangle(s))\n",
    "    captions['caption_lemma'] = captions['caption'].map(lemmatization)\n",
    "\n",
    "    # creating caption list\n",
    "    captions['caption_list'] = captions['caption_lemma'].map(\n",
    "        lambda row: word_tokenize(row.lower()))\n",
    "    \n",
    "    # removing stop words and punctuation\n",
    "    captions['caption_list'] = captions['caption_list'].map(lambda row: remove_stopwords(row))\n",
    "    \n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e83e84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tf dataframe\n",
    "def tf(col):\n",
    "    # creating tf-idf vector\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(col.values)\n",
    "    columns = vectorizer.get_feature_names()\n",
    "\n",
    "    # creating tf idf df\n",
    "    tf_idf_df = pd.DataFrame(X.toarray(), columns=columns)\n",
    "    \n",
    "    return tf_idf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47537216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tf idf dataframe\n",
    "def tf_idf(col):\n",
    "    # creating tf-idf vector\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(col.values)\n",
    "    columns = vectorizer.get_feature_names()\n",
    "\n",
    "    # creating tf idf df\n",
    "    tf_idf_df = pd.DataFrame(X.toarray(), columns=columns)\n",
    "    \n",
    "    return tf_idf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4580642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating tf-idf vectors\n",
    "def caption_tf_idf(captions, idf=True):\n",
    "    # creating a list of caption words\n",
    "    caption_words_list = []\n",
    "    for l in list(captions['caption_list'].values):\n",
    "        caption_words_list = caption_words_list + l\n",
    "\n",
    "    # removing duplicates \n",
    "    caption_words_list = list(set(caption_words_list))\n",
    "\n",
    "    # removing words with length <= 2\n",
    "    caption_words_list = [w for w in caption_words_list if len(w) > 2]\n",
    "\n",
    "    # converting the text to list\n",
    "    captions['caption_cleaned'] = captions['caption_list'].map(lambda lst: ' '.join(lst))\n",
    "    \n",
    "    # get tf idf vec\n",
    "    if idf:\n",
    "        caption_tf_idf = tf_idf(captions['caption_cleaned'])\n",
    "    else:\n",
    "        caption_tf_idf = tf(captions['caption_cleaned'])\n",
    "    caption_words_list = list(set(caption_words_list).intersection(set(caption_tf_idf.columns)))\n",
    "    caption_tf_idf = caption_tf_idf[caption_words_list]\n",
    "    \n",
    "    return caption_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b636cddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the cleaning functions\n",
    "def label_cleaning(data):\n",
    "    # subsetting labels only\n",
    "    labels = data[['labels']]\n",
    "    labels['labels'] = labels['labels'].astype(str).str.strip()\n",
    "    labels['labels'] = labels['labels'].map(lambda s: wrangle(s))\n",
    "    labels['labels_lemma'] = labels['labels'].map(lemmatization)\n",
    "\n",
    "    # creating labels list\n",
    "    labels['labels_list'] = labels['labels_lemma'].map(\n",
    "        lambda row: word_tokenize(row.lower()))\n",
    "    \n",
    "    # removing stop words and punctuation: NOT NEEDED FOR LABELS\n",
    "    # labels['labels_list'] = labels['labels_list'].map(lambda row: remove_stopwords(row))\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "995a70e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating tf-idf vectors\n",
    "def labels_tf_idf(labels, idf=True):\n",
    "    # creating a list of labels words\n",
    "    labels_words_list = []\n",
    "    for l in list(labels['labels_list'].values):\n",
    "        labels_words_list = labels_words_list + l\n",
    "\n",
    "    # removing duplicates \n",
    "    labels_words_list = list(set(labels_words_list))\n",
    "\n",
    "    # removing words with length <= 2\n",
    "    labels_words_list = [w for w in labels_words_list if len(w) > 2]\n",
    "\n",
    "    # converting the text to list\n",
    "    labels['labels_cleaned'] = labels['labels_list'].map(lambda lst: ' '.join(lst))\n",
    "    \n",
    "    # get tf idf vec\n",
    "    if idf:\n",
    "        labels_tf_idf = tf_idf(labels['labels_cleaned'])\n",
    "    else:\n",
    "        labels_tf_idf = tf(labels['labels_cleaned'])\n",
    "    labels_words_list = list(set(labels_words_list).intersection(set(labels_tf_idf.columns)))\n",
    "    labels_tf_idf = labels_tf_idf[labels_words_list]\n",
    "    \n",
    "    return labels_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4a1643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating tf-idf vectors\n",
    "def img_cap_tf_idf(img_cap, idf=True):\n",
    "    # creating a list of img_cap words\n",
    "    words_list = []\n",
    "    for l in list(img_cap['img_cap_list'].values):\n",
    "        words_list = words_list + l\n",
    "\n",
    "    # removing duplicates \n",
    "    words_list = list(set(words_list))\n",
    "\n",
    "    # removing words with length <= 2\n",
    "    words_list = [w for w in words_list if len(w) > 2]\n",
    "\n",
    "    # converting the text to list\n",
    "    img_cap['cleaned'] = img_cap['img_cap_list'].map(lambda lst: ' '.join(lst))\n",
    "    \n",
    "    # get tf idf vec\n",
    "    if idf:\n",
    "        img_cap_tf_idf = tf_idf(img_cap['cleaned'])\n",
    "    else:\n",
    "        img_cap_tf_idf = tf(img_cap['cleaned'])\n",
    "    words_list = list(set(words_list).intersection(set(img_cap_tf_idf.columns)))\n",
    "    img_cap_tf_idf = img_cap_tf_idf[words_list]\n",
    "    \n",
    "    return img_cap_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716cc719",
   "metadata": {},
   "source": [
    "## Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "806c99ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>img_url</th>\n",
       "      <th>caption</th>\n",
       "      <th>n_likes_1000</th>\n",
       "      <th>n_comments</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://instagram.flwo4-2.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>‚Äú100% of myself is nothing compared to 1% of t...</td>\n",
       "      <td>290k</td>\n",
       "      <td>28175</td>\n",
       "      <td>5 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://instagram.flwo4-2.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>Meet @azusa25nigo, the founder of @skate_girls...</td>\n",
       "      <td>88k</td>\n",
       "      <td>66716</td>\n",
       "      <td>6 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://instagram.flwo4-1.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>It takes courage to take the first step üèÉ. Jus...</td>\n",
       "      <td>243k</td>\n",
       "      <td>46306</td>\n",
       "      <td>6 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>https://instagram.flwo4-2.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>‚ÄúThe climate crisis is affecting my sport and ...</td>\n",
       "      <td>159k</td>\n",
       "      <td>87011</td>\n",
       "      <td>1 week ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://instagram.flwo4-2.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>‚ÄúPeople like to tell us what we can and can‚Äôt ...</td>\n",
       "      <td>252k</td>\n",
       "      <td>67646</td>\n",
       "      <td>1 week ago</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            img_url  \\\n",
       "0           0  https://instagram.flwo4-2.fna.fbcdn.net/v/t51....   \n",
       "1           1  https://instagram.flwo4-2.fna.fbcdn.net/v/t51....   \n",
       "2           2  https://instagram.flwo4-1.fna.fbcdn.net/v/t51....   \n",
       "3           3  https://instagram.flwo4-2.fna.fbcdn.net/v/t51....   \n",
       "4           4  https://instagram.flwo4-2.fna.fbcdn.net/v/t51....   \n",
       "\n",
       "                                             caption n_likes_1000  n_comments  \\\n",
       "0  ‚Äú100% of myself is nothing compared to 1% of t...         290k       28175   \n",
       "1  Meet @azusa25nigo, the founder of @skate_girls...          88k       66716   \n",
       "2  It takes courage to take the first step üèÉ. Jus...         243k       46306   \n",
       "3  ‚ÄúThe climate crisis is affecting my sport and ...         159k       87011   \n",
       "4  ‚ÄúPeople like to tell us what we can and can‚Äôt ...         252k       67646   \n",
       "\n",
       "          age  \n",
       "0  5 days ago  \n",
       "1  6 days ago  \n",
       "2  6 days ago  \n",
       "3  1 week ago  \n",
       "4  1 week ago  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the data\n",
    "data = pd.read_csv('Nike/nike_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd6e0c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(612, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246e8ea8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "captions = caption_cleaning(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f62bb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe4d32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_tf_idf_df = caption_tf_idf(captions)\n",
    "captions_tf_idf_df['caption'] = captions['caption']\n",
    "captions_tf_idf_df['caption_list'] = captions['caption_list']\n",
    "captions_tf_idf_df.to_csv('nike_caption_tf_idf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def920ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_tf_df = caption_tf_idf(captions, idf=False)\n",
    "captions_tf_df['caption'] = captions['caption']\n",
    "captions_tf_df['caption_list'] = captions['caption_list']\n",
    "captions_tf_df.to_csv('nike_caption_tf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239353de",
   "metadata": {},
   "source": [
    "## Image labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f927340d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>url</th>\n",
       "      <th>anger</th>\n",
       "      <th>joy</th>\n",
       "      <th>surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Furniture Picture frame Beard Standing Drawer ...</td>\n",
       "      <td>https://instagram.flwo4-2.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>VERY_UNLIKELY</td>\n",
       "      <td>VERY_UNLIKELY</td>\n",
       "      <td>VERY_UNLIKELY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sleeve Hat Music Workwear Cool Entertainment F...</td>\n",
       "      <td>https://instagram.flwo4-2.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>VERY_UNLIKELY</td>\n",
       "      <td>UNLIKELY</td>\n",
       "      <td>VERY_UNLIKELY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Footwear Jeans Shoe Wheel Sports equipment Ska...</td>\n",
       "      <td>https://instagram.flwo4-1.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>VERY_UNLIKELY</td>\n",
       "      <td>VERY_UNLIKELY</td>\n",
       "      <td>VERY_UNLIKELY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Joint Skin Shoe Arm Leg Shorts Purple Knee Com...</td>\n",
       "      <td>https://instagram.flwo4-2.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>VERY_UNLIKELY</td>\n",
       "      <td>VERY_LIKELY</td>\n",
       "      <td>VERY_UNLIKELY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Skin Lip Shoulder White Eyelash Organ Lingerie...</td>\n",
       "      <td>https://instagram.flwo4-2.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>VERY_UNLIKELY</td>\n",
       "      <td>VERY_UNLIKELY</td>\n",
       "      <td>VERY_UNLIKELY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>Trousers Shirt Fashion Flash photography Perfo...</td>\n",
       "      <td>https://scontent-hel3-1.cdninstagram.com/v/t51...</td>\n",
       "      <td>VERY_UNLIKELY</td>\n",
       "      <td>POSSIBLE</td>\n",
       "      <td>VERY_UNLIKELY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>Shoe Shorts Sneakers Flooring Floor Player Per...</td>\n",
       "      <td>https://scontent-hel3-1.cdninstagram.com/v/t51...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>Footwear Shorts Shoe Arm yoga pant Active pant...</td>\n",
       "      <td>https://scontent-hel3-1.cdninstagram.com/v/t51...</td>\n",
       "      <td>VERY_UNLIKELY</td>\n",
       "      <td>VERY_UNLIKELY</td>\n",
       "      <td>VERY_UNLIKELY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>Shorts Dance Entertainment Active pants Perfor...</td>\n",
       "      <td>https://scontent-hel3-1.cdninstagram.com/v/t51...</td>\n",
       "      <td>VERY_UNLIKELY</td>\n",
       "      <td>VERY_UNLIKELY</td>\n",
       "      <td>VERY_UNLIKELY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>Sleeve Gesture World Art Entertainment T-shirt...</td>\n",
       "      <td>https://scontent-hel3-1.cdninstagram.com/v/t51...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>612 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                labels  \\\n",
       "0    Furniture Picture frame Beard Standing Drawer ...   \n",
       "1    Sleeve Hat Music Workwear Cool Entertainment F...   \n",
       "2    Footwear Jeans Shoe Wheel Sports equipment Ska...   \n",
       "3    Joint Skin Shoe Arm Leg Shorts Purple Knee Com...   \n",
       "4    Skin Lip Shoulder White Eyelash Organ Lingerie...   \n",
       "..                                                 ...   \n",
       "607  Trousers Shirt Fashion Flash photography Perfo...   \n",
       "608  Shoe Shorts Sneakers Flooring Floor Player Per...   \n",
       "609  Footwear Shorts Shoe Arm yoga pant Active pant...   \n",
       "610  Shorts Dance Entertainment Active pants Perfor...   \n",
       "611  Sleeve Gesture World Art Entertainment T-shirt...   \n",
       "\n",
       "                                                   url          anger  \\\n",
       "0    https://instagram.flwo4-2.fna.fbcdn.net/v/t51....  VERY_UNLIKELY   \n",
       "1    https://instagram.flwo4-2.fna.fbcdn.net/v/t51....  VERY_UNLIKELY   \n",
       "2    https://instagram.flwo4-1.fna.fbcdn.net/v/t51....  VERY_UNLIKELY   \n",
       "3    https://instagram.flwo4-2.fna.fbcdn.net/v/t51....  VERY_UNLIKELY   \n",
       "4    https://instagram.flwo4-2.fna.fbcdn.net/v/t51....  VERY_UNLIKELY   \n",
       "..                                                 ...            ...   \n",
       "607  https://scontent-hel3-1.cdninstagram.com/v/t51...  VERY_UNLIKELY   \n",
       "608  https://scontent-hel3-1.cdninstagram.com/v/t51...           None   \n",
       "609  https://scontent-hel3-1.cdninstagram.com/v/t51...  VERY_UNLIKELY   \n",
       "610  https://scontent-hel3-1.cdninstagram.com/v/t51...  VERY_UNLIKELY   \n",
       "611  https://scontent-hel3-1.cdninstagram.com/v/t51...           None   \n",
       "\n",
       "               joy       surprise  \n",
       "0    VERY_UNLIKELY  VERY_UNLIKELY  \n",
       "1         UNLIKELY  VERY_UNLIKELY  \n",
       "2    VERY_UNLIKELY  VERY_UNLIKELY  \n",
       "3      VERY_LIKELY  VERY_UNLIKELY  \n",
       "4    VERY_UNLIKELY  VERY_UNLIKELY  \n",
       "..             ...            ...  \n",
       "607       POSSIBLE  VERY_UNLIKELY  \n",
       "608           None           None  \n",
       "609  VERY_UNLIKELY  VERY_UNLIKELY  \n",
       "610  VERY_UNLIKELY  VERY_UNLIKELY  \n",
       "611           None           None  \n",
       "\n",
       "[612 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_excel('Nike/NikeLabels.xlsx')\n",
    "labels.drop(0, inplace=True, axis=0)\n",
    "labels.reset_index(drop=True, inplace=True)\n",
    "labels.columns = [s.lower() for s in labels.columns]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de26fb04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_labels = label_cleaning(labels)\n",
    "image_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541b6c22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_tf_idf_df = labels_tf_idf(image_labels)\n",
    "image_tf_idf_df['labels'] = image_labels['labels']\n",
    "image_tf_idf_df['labels_list'] = image_labels['labels_list']\n",
    "print(image_tf_idf_df.head())\n",
    "image_tf_idf_df.to_csv('nike_label_tf_idf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349af6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tf_df = labels_tf_idf(image_labels, idf=False)\n",
    "image_tf_df['labels'] = image_labels['labels']\n",
    "image_tf_df['labels_list'] = image_labels['labels_list']\n",
    "print(image_tf_df.head())\n",
    "image_tf_df.to_csv('nike_label_tf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11e8873",
   "metadata": {},
   "source": [
    "## Image + Caption "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584baf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_caption = pd.concat([image_labels[['labels', 'labels_list']], captions[['caption', 'caption_list']]], axis=1)\n",
    "image_caption['img_cap_list'] = image_caption.apply(lambda row: row['labels_list'] + row['caption_list'], axis=1)\n",
    "image_caption.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a70da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check first entry\n",
    "assert len(image_caption.iloc[0, 1]) + len(image_caption.iloc[0, 3]) == len(image_caption.iloc[0, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd3306a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cap_tf_idf_df = img_cap_tf_idf(image_caption)\n",
    "img_cap_tf_idf_df['img_cap_list'] = image_caption['img_cap_list']\n",
    "print(img_cap_tf_idf_df.head())\n",
    "img_cap_tf_idf_df.to_csv('nike_img_cap_tf_idf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dae406",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cap_tf_df = img_cap_tf_idf(image_caption, idf=False)\n",
    "img_cap_tf_df['img_cap_list'] = image_caption['img_cap_list']\n",
    "print(img_cap_tf_df.head())\n",
    "img_cap_tf_df.to_csv('nike_img_cap_tf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5afc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert img_cap_tf_df.sum(axis=1).sum() - captions_tf_df.sum(axis=1).sum() - image_tf_df.sum(axis=1).sum() <= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29d1cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f19fb54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaa9be7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
